# Imaging the human hippocampus with optically-pumped magnetoencephalography

## Content
- [Stats](https://cravat5386.github.io/fdvr/20230101.html#stats)
- [Analysis (or my attempt at it)](https://cravat5386.github.io/fdvr/20230101.html#analysis)
- [Edits and Addendums](https://cravat5386.github.io/fdvr/20230101.html#addendums)

## Stats:
Authors: Daniel N. Barry, Tim M. Tierney, Niall Holmes, Elena Boto, Gillian Roberts, James Leggett, Richard Bowtell, Matthew J. Brookes, Gareth R. Barnes, Eleanor A. Maguire
Publishing Date: Dec 2019
Journal: NeuroImage
DOI: [https://doi-org.myaccess.library.utoronto.ca/10.1016/j.neuroimage.2019.116192](https://doi-org.myaccess.library.utoronto.ca/10.1016/j.neuroimage.2019.116192)

## Analysis:
The main thing I wanted to get out of this article was the OP-MEG system - the setup, pros and cons, how noise was reduced, how the signal was processed, etc. This information would be helpful in deciding the input system of an FDVR machine - that is, how an FDVR machine would get brain signals for how a user wants to move. As of now, there are a variety of different methods I've been considering, including MEG, but also EEG, NIRS, and (it's a bit of a stretch, but) fMRI.

### Setup
For this research, the OP-MEG sensors were placed using 3D-printed casts custom-designed for each participant. While no doubt allowing for optimal localization of sources of activity within the brain, this option is not feasible for generalization and commercialization. In addition, for an FDVR machine, there should be easier ways to localize the proper activity sources through calibration, though how that would work and how the sensors should be targeted remains to be seen.

There were a few reference sensors placed around 10 cm away from the patient's head, and these were used to calibrate field-nulling coils to reduce noise from the earth's magnetic field. I think this is a very interesting idea - I'll have to check on its feasibility later. Looking at the figures of the apparatus, this portion of the setup appears to be extremely bulky and unwieldy; however, if this could be optimized and shrunken down, it would no doubt be a useful addition to any device.

The dynamic range was set to 1.5 nT with 0-135 Hz bandwidth, and digitized by a 16-bit instrument. The signal was sampled at 1200 Hz.

### Data Analysis
There was a bunch of data analysis stuff that I'll have to research more on later. First, to transform the data into a comparable state, they used a scalar linearly-constrained, minimum-variance beamformer algorithm (I'll definitely have to look into that later) using the Nolte single shell forward model in SPM12. Beamformer weightings were estimated using a covariance window for the whole trial using the 5th order bi-directional Butterworth filter. This generated a parametric map of power changes, which was then transformed into MNI space (nonlinearly) and then smoothed with a Gaussian kernel.

*Edit: See beamformer addendum*

Yeah... so...

The gist of the rest of it (without the details on how any of that works) is that they wanted to figure out which parts of the brain (specifically with volume pixels or voxels in their scans) was important to their task of spatial visualization. After transforming the data like that, they performed a bunch of tests and filters commonly used in MRI and fMRI analysis that I won't really get into.

As for FDVR, I can see how this stuff might be useful for the calibration side of things, but I have no idea what most of them mean, so I'll probably put more information into the appendices soon.

### Pros and Cons
This will mainly be analyzing the discussion section of the paper, but will also comment on some other related stuff that I want to talk about.

So as per the paper, they were able to get good results that were verified by SQUID-MEG measurements and correpsond to previous fMRI studies. They were also able to get some measurements that were not specific to their experiment, but were expected based on literature. One of the cool things that the authors were able to do was to use a "limited" amount of sensors - only 20-30, to get a rather spatially precise measurement of multiple areas of the brain. Although more sensors would result in more precision, this must be balanced with costs, space, and the extra compute required.

Another thing to note is that the main competitor for MEGs are EEGs - since (to quote everyone's high school physics teacher) electricity and magnetism are two sides of the same coin. However, while EEGs have so many artefacts - sweating, muscle contractions, movements, etc, there aren't as many in MEGs (this is not explicitly stated by the paper, but implied). The only explicitly mentioned thing is that artefacts from muscle contractions are 10 times lower in MEGs comparted to EEGs. MEGs also provide enhanced spatial resolution, and require less sensors (all the EEG studies mentioned use high-density EEG).

Although EEGs do have higher depth sensitivity than MEGs, this should not be a problem for FDVR applications since the motor cortex is near the surface of the brain.

## Addendums
### Beamformer Algorithms
The below draws heavily on Source \[1\].

MEGs measure changes in the magnetic field that are caused by neuronal activity near the scalp. However, localizing where this activity occurs is problematic, since there is a potentially infinite number of combinations and regions these fields could be coming from. As per the paper, you have this mathematical representation of the problem:

Given the time-series of vectors
$$ \mathbf{Y}(t) = (Y_1 (t), \dots Y_n (t))^T \in \mathbb{R}^n, t = t_j, 1 \le j \le J $$
from n sensors, and p candidate sources at $r_k, 1 \le k \le p$ in the brain, how do you find which $r_k$ is significant based on the model
$$\mathbf{Y}(t) = \sum_{k=1}^{p} H_k \mathbf{m}_k (t) + \epsilon (t)$$

In this case, $H_k$ is the *lead field matrix* of $r_k$, which is how a source at $r_k$ would affect measurements at each of the sensors, $\mathbf{m}_k (t)$ is the moment at time $t$ and location $r_k$, and $\epsilon$ is the white noise.

There are a bunch of approaches that are used to solve this problem - and the ones used by the authors of the main paper, linearly-constrained, minimum-variance beamforming, is one of them.

To find regions of interest using this algorithm, one first does a projection step, then a mapping step. The projection step finds an optimal $W$ matrix that minimizes the covarience of $W^T Y(t_j), 1 \le j \le J$. However, $W^T H_k == I$. This projection gives an optimal trace $$\hat{S}_k = tr([H^T_k \hat{C}^{-1} H_k]^{-1})$$ Here, $\hat{C}$ is the covariance estimator for the sensors. Note that this step tries to estimate the desired signal from each location, while minimizing the contributions of other signals and noise.

Then for the mapping step, the activity at each area is divided by $\sigma^2_0 tr([H^T_k H_k]^{-1})$, which is an estimate of noise. Essentially, these steps seek to maximize signal for each location, then create a map of the signal-to-noise ratio.

There is more complicated math, but this is just the basics.

### Addendum Sources
\[1\] Zhang, J., Liu, C., & Shen, X. (2015). On Linearly Constrained Minimum Variance Beamforming. *Journal of Machine Learning Research, 16*, 2099â€“2145. 

## Edit History
Jan 1, 2023 - First Draft
Jan 6, 2023 - Beamformer Algorithm Addendum